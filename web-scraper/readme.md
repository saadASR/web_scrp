# ğŸ” Web Scraper Pro

Web Scraper Pro is a **modern and secure web scraper** built with **Node.js, Express, and Cheerio**, featuring a responsive user interface and a clean MVC-based architecture.

---

## âœ¨ Main Features

* ğŸ¯ **Full scraping**: titles, paragraphs, images, links, metadata
* ğŸš€ **Optimized performance**: 10-minute caching system
* ğŸ” **Advanced security**: URL validation, anti-SSRF protection, rate limiting
* ğŸ“Š **Automatic statistics**: element counting & word count
* ğŸ¨ **Modern UI**: gradients, animations, smooth interactions
* ğŸ“± **Responsive design**: works on mobile, tablet, and desktop
* ğŸ“ **Advanced logs**: Winston logging system
* âš¡ **Clean architecture**: routes, controllers, services, utils

---

## ğŸ“‹ Requirements

* Node.js **>= 16.0.0**
* npm **>= 8.0.0**

---

## ğŸš€ Installation

1. Clone the project:

```bash
git clone https://github.com/NezarEa/web-scraper.git
cd web-scraper-pro
```

2. Install dependencies:

```bash
npm install
```

3. Create the `.env` file:

```bash
cp .env.example .env
```

4. Create the logs folder:

```bash
mkdir logs
```

5. Start the server:

```bash
npm run dev   # Development mode
npm start     # Production mode
```

6. Open the application:

```
http://localhost:3000
```

---

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ server.js
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ scraper.js
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ scraperController.js
â”œâ”€â”€ services/
â”‚   â””â”€â”€ scrapingService.js
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ validators.js
â”‚   â””â”€â”€ logger.js
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js
â”œâ”€â”€ logs/
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ“¡ API

### **POST /api/scrape**

Scrapes a URL and returns extracted data.

**Body:**

```json
{
  "url": "https://example.com"
}
```

**Response includes:**

* Title
* Metadata
* Headings
* Paragraphs
* Links
* Images
* Statistics
* Cache status

---

### **GET /api/cache/stats**

Returns cache statistics.

### **DELETE /api/cache/clear**

Clears the cache.

---

## ğŸ›¡ï¸ Built-in Security

* Strict URL validation
* SSRF protection
* Blocking private & local IPs
* Rate limiting (20 requests / 15 min)
* 10-second timeout
* 10MB max response size
* HTML escaping (XSS protection)

---

## ğŸ”„ Limitations

* Does not support JavaScript-rendered websites
* Cannot bypass CAPTCHAs
* 10-second maximum timeout
* Response size limited to 10MB

---

## ğŸ‘¨â€ğŸ’» Author

**ElAyachi Nezar**
GitHub: **@NezarEa**
